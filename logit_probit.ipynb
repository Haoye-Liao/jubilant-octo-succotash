{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logit_probit:\n",
    "    import scipy as sp\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    import statsmodels as sm\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # By this command, all annoying warning signs would be ignored.\n",
    "    \n",
    "    def __init__(self,x,y,distr,Add_intercept):  \n",
    "        if Add_intercept=='True':\n",
    "            self.x=sm.add_constant(x)\n",
    "        elif Add_intercept=='False':\n",
    "            self.x=x\n",
    "        # The Matrix of X, one can choose whether to add a constant term.\n",
    "        self.y=y \n",
    "        # Vector of y.\n",
    "        if distr == 'probit':\n",
    "            self.distr = sp.stats.norm\n",
    "        elif distr == 'logit':\n",
    "            self.distr = sp.stats.logistic # Here, one can choose which model (logit or probit) to use.\n",
    "        \n",
    "    def loglikelihood(self,parm):\n",
    "        mu_vec=np.array((-np.inf,0,np.inf)) \n",
    "        # Since logit and probit are the special cases of ordered logit and probit, I use the similar method by setting the threhold (only zero here) to estimate them.\n",
    "        inner=self.distr.cdf(mu_vec[[self.y.astype('int')+1]]-np.dot(self.x,parm),0,1)-self.distr.cdf(mu_vec[[self.y.astype('int')]]-np.dot(self.x,parm),0,1)\n",
    "        return -np.sum(np.log(inner))\n",
    "    \n",
    "    def params(self):\n",
    "        def loglikelihood(parm):\n",
    "            mu_vec=np.array((-np.inf,0,np.inf)) \n",
    "            inner=self.distr.cdf(mu_vec[[self.y.astype('int')+1]]-np.dot(self.x,parm),0,1)-self.distr.cdf(mu_vec[[self.y.astype('int')]]-np.dot(self.x,parm),0,1)\n",
    "            return -np.sum(np.log(inner))\n",
    "             \n",
    "        return sp.optimize.fmin_bfgs(loglikelihood,np.array([0,0,0]))\n",
    "    \n",
    "    def std(self):\n",
    "        def loglikelihood(parm):\n",
    "            mu_vec=np.array((-np.inf,0,np.inf)) \n",
    "            inner=self.distr.cdf(mu_vec[[self.y.astype('int')+1]]-np.dot(self.x,parm),0,1)-self.distr.cdf(mu_vec[[self.y.astype('int')]]-np.dot(self.x,parm),0,1)\n",
    "            return -np.sum(np.log(inner))\n",
    "        self.res=sp.optimize.minimize(loglikelihood,np.array([0,0,0]), method='bfgs')\n",
    "        # Since I need to use the Hessian matrix to get the information matrix, I switch to the scipy.minimize moduel, which can easily get it.\n",
    "        self.std=np.sqrt(np.diagonal(self.res.hess_inv))\n",
    "        \n",
    "        return np.sqrt(np.diagonal(self.res.hess_inv))\n",
    "    \n",
    "    def summary(self):\n",
    "        def loglikelihood(parm):\n",
    "            mu_vec=np.array((-np.inf,0,np.inf)) \n",
    "            inner=self.distr.cdf(mu_vec[[self.y.astype('int')+1]]-np.dot(self.x,parm),0,1)-self.distr.cdf(mu_vec[[self.y.astype('int')]]-np.dot(self.x,parm),0,1)\n",
    "            return -np.sum(np.log(inner))\n",
    "        self.res=sp.optimize.minimize(loglikelihood,np.array([0,0,0]), method='bfgs')\n",
    "        self.std=np.sqrt(np.diagonal(self.res.hess_inv))\n",
    "        self.z=self.res.x/self.std # Calculate the Z-value.\n",
    "        self.ci_up=1.96*self.std+self.res.x # Upper bound of the 95% CI.\n",
    "        self.ci_low=-1.96*self.std+self.res.x # Lower bound of the 95% CI.\n",
    "        self.data = {'coef.':self.res.x,'std.errors':self.std,'z-value':self.z,'[0.025':self.ci_low,'0.975]':self.ci_up} \n",
    "        row=['_cons']\n",
    "        for i in range(self.x.shape[1]-1):\n",
    "            row.append('x'+str(i+1))\n",
    "        # By this for loop, the row indices will be x1,x2,...,xN, where N is the number of explanatory variables.\n",
    "        return pd.DataFrame(self.data, index=row) \n",
    "    \n",
    "    def bootstrap(self,niter, method):\n",
    "        \n",
    "        def logl(parm,x,y): # Here, I rewrite the log-likelihood function by adding x & y as the parameters in this function because in the below part I need to use this function in bootstrap estimation\n",
    "            mu_vec=np.array((-np.inf,0,np.inf)) \n",
    "            inner=self.distr.cdf(mu_vec[[y.astype('int')+1]]-np.dot(x,parm),0,1)-self.distr.cdf(mu_vec[[y.astype('int')]]-np.dot(x,parm),0,1)\n",
    "            return -np.sum(np.log(inner))\n",
    "            \n",
    "        def estimate(x,y):\n",
    "            return sp.optimize.minimize(logl,np.array([0,0,0]),args=(x,y),method='bfgs')\n",
    "        \n",
    "        self.Bsample_x=np.zeros(self.x.shape)\n",
    "        self.Bsample_y=np.zeros(self.y.shape)\n",
    "        \n",
    "        self.beta_B=np.zeros([niter,self.x.shape[1]])\n",
    "       \n",
    "        \n",
    "        for k in range(niter): # 'niter' is the number of iterations.\n",
    "\n",
    "            for i in range(len(self.y)):\n",
    "                j=np.random.uniform(0,len(self.y),1).astype('int') # Transform it into integer is because j is the index.\n",
    "                self.Bsample_x[i,:]=self.x[j,:]\n",
    "                self.Bsample_y[i]=self.y[j]\n",
    "            # By this for-loop, bootstrap samples are constructed.\n",
    "\n",
    "            self.beta_B[k,:]=estimate(x=self.Bsample_x, y=self.Bsample_y).x \n",
    "            # Then, I estimate beta by different bootstrap samples for 'niter' times and store these estimates in a matrix.\n",
    "        \n",
    "            \n",
    "        self.Res = self.beta_B.mean(axis=0) \n",
    "        # Get the results of estimation by taking the sample average columns by columns in the bootstrap beta matrix.\n",
    "        self.std = self.beta_B.std(axis=0)\n",
    "        # Also can calculate the bootstrap standard errors.\n",
    "        \n",
    "        # (1) I construst the 95% CI by normal approximation method.\n",
    "        self.res=estimate(x=self.x, y=self.y).x # Estimates by using the original dataset.\n",
    "        self.z_B=self.Res/self.std\n",
    "        self.ci_B_up=1.96*self.std+self.res # Upper bound of the 95% CI.\n",
    "        self.ci_B_low=-1.96*self.std+self.res # Lower bound of the 95% CI.\n",
    "        \n",
    "        # (2) I calculate the 95% CI of bootstrap by simply taking the 2.5% and 97.5% percentiles.\n",
    "        obs = np.array([niter*0.025],dtype='i')\n",
    "        self.ci_B_up1=np.zeros(self.x.shape[1])\n",
    "        self.ci_B_low1=np.zeros(self.x.shape[1])\n",
    "        for i in range(self.x.shape[1]):\n",
    "    \n",
    "            self.ci_B_up1[i]=np.percentile(self.beta_B[:,i], 97.5)\n",
    "                              \n",
    "            self.ci_B_low1[i]=np.percentile(self.beta_B[:,i], 2.5)\n",
    "        \n",
    "        # Then, I use the 'if' to choose which method to use.\n",
    "        \n",
    "        row=['_cons']\n",
    "        for i in range(self.x.shape[1]-1):\n",
    "            row.append('x'+str(i+1))\n",
    "        # By this for loop, the row indices will be x1,x2,...,xN, where N is the number of explanatory variables.\n",
    "    \n",
    "        if method == 'percentile':\n",
    "            self.data_B = {'coef.':self.Res,'std.errors':self.std,'[0.025':self.ci_B_low1,'0.975]':self.ci_B_up1} \n",
    "            return pd.DataFrame(self.data_B, index=row)\n",
    "        elif method == 'normal':\n",
    "            self.data_B = {'coef.':self.Res,'std.errors':self.std,'z-value':self.z_B,'[0.025':self.ci_B_low,'0.975]':self.ci_B_up} \n",
    "            return pd.DataFrame(self.data_B, index=row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of the Bootstrap sampling in above class:\n",
    "- In Bootstrap sampling, let $\\hat \\beta$ denote the estimated value calculated with the original dataset, $\\bar \\beta$ denote the Bootstrap-estimated value. Let $\\hat \\beta_i$ be the value of the statistic from the $i$-th bootstrap sample. \n",
    "- Then, $$\\hat{se}=\\bigg(\\frac{1}{B-1} \\sum_{i=1}^{B}(\\hat \\beta_i-\\bar \\beta)^2\\bigg)^{1/2}$$\n",
    "- where, $$\\bar \\beta=\\frac{1}{B}\\sum_{i=1}^{B}\\hat \\beta_i$$\n",
    "- The normal-approximation method yields the 95\\% confidence intervals $$\\bigg[\\hat \\beta-z_{0.025}\\hat{se},\\hat \\beta+z_{0.025}\\hat{se}\\bigg]$$ where $z_{0.025}(1.96)$ is the 2.5\\% quantile of the standard normal distribution.\n",
    "- The percentile method yields the confidence intervals $$\\bigg[\\hat \\beta_{0.975}^{*},\\hat \\beta_{0.025}^{*}\\bigg]$$ where $\\hat \\beta_{p}^{*}$ is the $p$-th quantile of the bootstrap distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
